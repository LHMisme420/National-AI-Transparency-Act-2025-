# National AI Transparency and Accountability Act of 2025


A BILL
To establish minimum transparency and accountability requirements for high-impact artificial intelligence systems deployed in the United States, and for other purposes.

Be it enacted by the Senate and House of Representatives of the United States of America in Congress assembled,

### SECTION 1. SHORT TITLE.
This Act may be cited as the “National AI Transparency and Accountability Act of 2025”.

### SECTION 2. FINDINGS.
Congress finds that:
1. Artificial intelligence systems increasingly make or influence high-stakes decisions affecting Americans’ access to housing, credit, employment, education, healthcare, and criminal justice.
2. Lack of transparency and independent oversight has led to documented harms including algorithmic discrimination and uncontrolled high-risk outcomes.
3. The United States can protect civil rights and promote innovation by requiring basic transparency and accountability without banning any specific technology.

### SECTION 3. DEFINITIONS.
(a) Covered AI System — Any artificial intelligence system that:
   (1) is offered as a commercial product or service in the United States; and
   (2) directly makes or is a controlling factor in making a consequential decision in one or more of the following domains: credit, education, employment, housing, insurance, healthcare, legal sentencing or policing, or public benefits eligibility.

(b) Developer — Any person or entity that designs, trains, or substantially modifies a covered AI system.

(c) Deployer — Any person or entity that uses a covered AI system in making consequential decisions affecting natural persons in the United States.

### SECTION 4. TRANSPARENCY REQUIREMENTS.
Within 180 days of enactment and annually thereafter, every developer and deployer of a covered AI system shall publish and keep current on a publicly accessible website:
(1) A plain-language Model Card describing:
   - Intended and reasonably foreseeable uses
   - Training data summary (domains, time periods, and filtering methods)
   - Known limitations and risk-mitigation measures
(2) An Impact Assessment documenting:
   - Pre-deployment testing results for accuracy, bias, and robustness across protected classes
   - Human oversight procedures
   - Complaint and appeal mechanisms available to affected individuals
(3) A machine-readable System Card following the latest NIST AI Risk Management Framework or equivalent standard.

### SECTION 5. NOTICE REQUIREMENT.
Any deployer using a covered AI system for a consequential decision shall, at or before the time the decision is communicated to the affected individual, provide clear and accessible notice that an automated system played a material role and include a link or QR code to the documents required in Section 4.

### SECTION 6. INDEPENDENT AUDIT REQUIREMENT.
(a) Every covered AI system with more than 1,000,000 monthly active users or that affects more than 100,000 individuals annually in consequential decisions shall undergo an independent third-party audit at least once every two years.
(b) Audits shall be conducted by entities accredited by NIST or ANSI and shall evaluate compliance with Sections 4 and 5 and test for disparate impact under applicable federal civil-rights laws.
(c) Audit reports shall be submitted to the Federal Trade Commission and made public in redacted form protecting legitimate trade secrets.

### SECTION 7. ENFORCEMENT.
(a) The Federal Trade Commission shall have primary enforcement authority and may treat violations of this Act as unfair or deceptive acts or practices under section 5 of the FTC Act.
(b) State attorneys general may bring civil actions in federal district court to enforce compliance.
(c) Private right of action: Any individual harmed by a violation may bring a civil action for injunctive relief and reasonable attorney fees.

### SECTION 8. CIVIL PENALTIES.
Violations may be punished by civil penalties of not more than $50,000 per day per covered AI system, adjusted annually for inflation.

### SECTION 9. SAFE HARBOR AND REGULATORY SANDBOX.
(a) Developers and deployers who fully comply with Sections 4–6 and follow the latest NIST AI Risk Management Framework shall receive a rebuttable presumption of compliance with Section 5 of the FTC Act with respect to the deployed system.
(b) The FTC shall establish a voluntary regulatory sandbox program for innovative systems that do not yet meet all requirements.

### SECTION 10. NO PREEMPTION OF STRONGER STATE LAWS.
Nothing in this Act shall be construed to preempt any state law that provides greater transparency or accountability protections.

### SECTION 11. SEVERABILITY.
If any provision of this Act is held invalid, the remainder shall not be affected.

### SECTION 12. EFFECTIVE DATE.
This Act shall take effect 180 days after enactment, with staggered compliance deadlines for small businesses as determined by the FTC.


